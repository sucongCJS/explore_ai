{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结构\n",
    "- dnn_model(X, Y, layers_dims, learning_rate, num_iterations) return parameters: 模型, 传入属性, 预测值, 网络层结构等, 返回训练好的参数\n",
    "    - initialize_parameters_deep(layer_dims) return parameters: 初始化网络层参数\n",
    "    - L_model_forward(X, parameters) return AL, caches: 根据属性和参数, 计算预测值, 可以控制每一层的激活函数是什么, 返回AL为每个样本的预测值, caches为每一层的(A_prev, W, b, Z)\n",
    "        - linear_activation_forward(A_prev, W, b, activation) return A, cache: 非线性前向传播, 返回值A是本层的输出Z经过激活后的值, cache的值是(A_prev, W, b, Z)\n",
    "            - linear_forward(A_prev, W, b) return Z, cache: 线性前向传播, A_prev是上一层的输出, W, b是本层的, 通过A_prev, W, b算出本层的输出Z(未激活), 返回的cache值是(A_prev, W, b)\n",
    "    - compute_cost(AL, Y): 计算成本函数\n",
    "    - L_model_backward(AL, Y, caches):\n",
    "        - linear_activation_backward(dA, cache, activation) return dA_prev, dW, db:\n",
    "            - linear_backward(dZ, cache) return dA_prev, dW, db: \n",
    "    - update_parameters(parameters, grads, learning_rate): 调整参数\n",
    "- predict(X,parameters) return p: 传入训练好的参数来预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载我们自定义的工具函数\n",
    "from testCases import *\n",
    "from dnn_utils import *\n",
    "\n",
    "# 设置一些画图相关的参数\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该函数用于初始化所有层的参数w和b\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    layer_dims -- 这个list列表里面，包含了每层的神经元个数。\n",
    "    例如，layer_dims=[5,4,3]，表示输入层有5个神经元，隐藏层第一层有4个，隐藏层第二层有3个神经元\n",
    "    \n",
    "    返回值:\n",
    "    parameters -- 这个字典里面包含了每层对应的已经初始化了的W和b。\n",
    "    例如，parameters['W1']装载了第一层的w，parameters['b1']装载了第一层的b\n",
    "    \"\"\"\n",
    "    L = len(layer_dims)  # 一共有多少层\n",
    "    parameters = {}\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # 遍历每一层，为每一层的W和b进行初始化\n",
    "    for i in range(1, L):  # layer_dims[0] 表示输入有多少个神经元, 只需要初始化隐藏层的参数\n",
    "        # 维度部分可参考笔记: 第i层Wi的维度是(n[i] , n[i-1])\n",
    "#         parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) / np.sqrt(layer_dims[i-1])  # 一行是一个神经元对每个输入的权重, 缩小的程度应该和每个神经元有多少个权重有关, 所以是除以np.sqrt(layer_dims[i-1]), 而不是np.sqrt(layer_dims[i])\n",
    "        parameters['W'+str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1])  # test\n",
    "        parameters['b'+str(i)] = np.zeros((layer_dims[i], 1))\n",
    "        \n",
    "        # 核对一下W和b的维度是我们预期的维度\n",
    "        assert(parameters['W'+str(i)].shape == (layer_dims[i], layer_dims[i - 1]))\n",
    "        assert(parameters['b'+str(i)].shape == (layer_dims[i], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
      " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
      " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
      " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
      " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
      " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# 看看效果\n",
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播, Z^[i] = W^[i]A^[i-1]+b^[i]\n",
    "def linear_forward(A_prev, W, b):   \n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (A_prev, W, b) # 将这些变量保存起来，因为后面进行反向传播时会用到它们\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现公式 A[i]=g(Z[i])，g代表激活函数，使上面的线性前向传播就变成非线性前向传播。激活函数sigmoid和relu定义在dnn_utils.py中\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev -- 上一层得到的A，输入到本层来计算Z和本层的A。第一层时A_prev就是特征输入X\n",
    "    W -- 本层相关的W\n",
    "    b -- 本层相关的b\n",
    "    activation -- 两个字符串，\"sigmoid\"或\"relu\"，指示该层应该使用哪种激活函数\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "#     cache = (linear_cache, Z) # 缓存一些变量，后面的反向传播会用到它们\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个完整的前向传播过程。前向传播一共有L层，前面L-1层用激活函数relu，最后一层用sigmoid\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    X -- 输入的特征数据\n",
    "    parameters -- 这个list列表里面包含了每一层的参数w和b\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters)//2  # 为什么是一半呢？因为列表是这样的[w1,b1,w2,b2...wl,bl],里面的w1和b1代表了一层\n",
    "    \n",
    "    A_prev = X\n",
    "    for i in range(1, L):  # 先进行L-1次前向传播, 最后一次在下面, i从1开始\n",
    "        A = linear_activation_forward(A_prev, \n",
    "                                           parameters['W'+str(i)],\n",
    "                                           parameters['b'+str(i)],\n",
    "                                          \"relu\")\n",
    "        A_prev = A\n",
    "        \n",
    "    # 进行最后一层的前向传播，这一层的激活函数是sigmoid。得出的AL就是y'预测值\n",
    "    AL = linear_activation_forward(A_prev, \n",
    "                                   parameters['W'+str(L)], \n",
    "                                   parameters['b'+str(L)], \n",
    "                                  \"sigmoid\")\n",
    "    \n",
    "    assert(AL.shape == (1, X.shape[1]))\n",
    "    \n",
    "    return AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面已经完成了前向传播了。下面这个函数用于计算成本（单个样本时是损失，多个样本时是成本）\n",
    "# 通过每次训练的成本我们就可以知道当前神经网络学习的程度好坏。\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    AL -- 预测值\n",
    "    Y -- 真实值\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1/m) * np.sum(np.multiply(Y, np.log(AL) + np.multiply(1-Y, np.log(1-AL))))\n",
    "    cost = np.squeeze(cost)  # 确保cost是一个数值而不是一个数组的形式\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    dZ -- 后面一层的dZ\n",
    "    cache -- 前向传播时我们保存下来的关于本层的一些变量\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
