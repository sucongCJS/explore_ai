{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载我们自定义的工具函数\n",
    "from testCases import *\n",
    "from dnn_utils import *\n",
    "\n",
    "# 设置一些画图相关的参数\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) \n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该函数用于初始化所有层的参数w和b\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    layer_dims -- 这个list列表里面，包含了每层的神经元个数。\n",
    "    例如，layer_dims=[5,4,3]，表示输入层有5个神经元，隐藏层第一层有4个，隐藏层第二层有3个神经元\n",
    "    \n",
    "    返回值:\n",
    "    parameters -- 这个字典里面包含了每层对应的已经初始化了的W和b。\n",
    "    例如，parameters['W1']装载了第一层的w，parameters['b1']装载了第一层的b\n",
    "    \"\"\"\n",
    "    L = len(layer_dims)  # 一共有多少层\n",
    "    parameters = {}\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # 遍历每一层，为每一层的W和b进行初始化\n",
    "    for i in range(1, L):  # layer_dims[0] 表示输入有多少个神经元, 只需要初始化隐藏层的参数\n",
    "        # 维度部分可参考笔记: 第i层Wi的维度是(n[i] , n[i-1])\n",
    "#         parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) / np.sqrt(layer_dims[i-1])  # 一行是一个神经元对每个输入的权重, 缩小的程度应该和每个神经元有多少个权重有关, 所以是除以np.sqrt(layer_dims[i-1]), 而不是np.sqrt(layer_dims[i])\n",
    "        parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1])  # test\n",
    "        parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "        \n",
    "        # 核对一下W和b的维度是我们预期的维度\n",
    "        assert(parameters['W' + str(i)].shape == (layer_dims[i], layer_dims[i - 1]))\n",
    "        assert(parameters['b' + str(i)].shape == (layer_dims[i], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.72642933 -0.27358579 -0.23620559 -0.47984616  0.38702206]\n",
      " [-1.0292794   0.78030354 -0.34042208  0.14267862 -0.11152182]\n",
      " [ 0.65387455 -0.92132293 -0.14418936 -0.17175433  0.50703711]\n",
      " [-0.49188633 -0.07711224 -0.39259022  0.01887856  0.26064289]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.55030959  0.57236185  0.45079536  0.25124717]\n",
      " [ 0.45042797 -0.34186393 -0.06144511 -0.46788472]\n",
      " [-0.13394404  0.26517773 -0.34583038 -0.19837676]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# 看看效果\n",
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前向传播, Z^[i] = W^[i]A^[i-1]+b^[i]\n",
    "def linear_forward(A, W, b):   \n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b) # 将这些变量保存起来，因为后面进行反向传播时会用到它们\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现公式 A[i]=g(Z[i])，g代表激活函数，使上面的线性前向传播就变成非线性前向传播。激活函数sigmoid和relu定义在dnn_utils.py中\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev -- 上一层得到的A，输入到本层来计算Z和本层的A。第一层时A_prev就是特征输入X\n",
    "    W -- 本层相关的W\n",
    "    b -- 本层相关的b\n",
    "    activation -- 两个字符串，\"sigmoid\"或\"relu\"，指示该层应该使用哪种激活函数\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "#     cache = (linear_cache, Z) # 缓存一些变量，后面的反向传播会用到它们\n",
    "        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数构建了一个完整的前向传播过程。这个前向传播一共有L层，前面的L-1层用的激活函数是relu，最后一层使用sigmoid。\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    参数:\n",
    "    X -- 输入的特征数据\n",
    "    parameters -- 这个list列表里面包含了每一层的参数w和b\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
